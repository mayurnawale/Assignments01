1)
The definition of a feature is a part of the face, a quality, a special attraction, article or a major film showing in the theatre. An example of feature is a nose.

2)
when given set of input features involves transforming to generate a new set of more power ful features which going to be used for the prediction.

3)
We use this categorical data encoding technique when the features are nominal,which means they do not have any order.in one hot encoding, for each level of a categorical feature,we create a new variable.each category is mapped with a binary variable containing either 0 or 1.now we have to one-hot encode this data.

4)
 divide the raw source data into equal intervals
 
5)
The wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain.in addition, the feature subsets selected by the wrapper are significantly smaller than the original subsets used by the learning algorithms, thus producing more comprehensible models.

6)
Strong relevance implies that the feature is indispensable in the sense that it cannot be removed without loss of prediction accuracy.they are irrelevant otherwise,there features can never contribute to prediction accuracy, by de nition.


7)
A defconst is redundant if the name is already defined either with a syntactically identical defconst event or one that defines it to have the same value.
if two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features.

8)
The Jaccard distance measures the similarity of the two data set items as the intersection of those items divided by the union of the data items. When p = 2, Minkowski distance is same as the Euclidean distance. When p = 1, Minkowski distance is same as the Manhattan distance.

9)
Euclidean distance is the shortest path between source and destination which is a straight line.but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines.

10)
feature transformation:transformation of data to improve the accuracy of the algorithm. feature selection:removing unnecessary features.

11)
ii)
A hybrid feature selection method is proposed for classification in small sample size data sets.a few candidate feature subsets are generated since their number corresponds to the number of instances.cooperative feature subset search is proposed with a classifier algorithm for the wrapper step.

iii)
Silhouette width (SW) of a cell i is the distance of cell i from all of the cells within the same cluster subtracted by the distance of cell i from cells in a nearest but different cluster, normalized by the maximum of these two values.

iv)
A receiver operating characteristic curve, or ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.




