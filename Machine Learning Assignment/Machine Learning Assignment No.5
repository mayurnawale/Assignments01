1)
Key task of machine learning :
-Data gathering
-Data preprocessing
-Exploratory data analysis
-Feature engineering
-Training machine learning model
-Testing and matching
-Density estimation

-It is a process of transforming raw data into an understandable format.It is important step in data mining as we cannot work with raw data.Raw data is incomplete and inconsistent.so that quality of data should be checked before applying machine learning and data mining algorithms.

2)
-Quantitative data is anything that can be counted or measured.
-this data is in the numerical form.
-this data is in measurable form.
-for eg.age,weight,size,weight.

Qualitative data is discriptive and regards phenomenon which can be observed not measured.
-this data is in discriptiv and in text format.
-this data is not in measurable form. 
-for eg.colour,religion,gender,nationality.

3)
The attributes of machine learning data types-
-The ability to perform automated data visualization.
-Automation at its best.
-Customer engagement like never before.
-The ability to take efficiency to the next level when merged with IoT.
-The ability to change the mortgage market.
-Accurate data analysis.
    
4)
various causes of machine learning data issues:
-Poor quality of data
-Underfitting of training data
-Overfitting of training data
-Lack of training data.

various ramifications of machine learning data issues:
-Understanding Which Processes Need Automation.
-Lack of Quality Data.
-Inadequate Infrastructure.
-Implementation.
-Lack of Skilled Resources.
    

5)
Cramer-V is a very useful data exploration technique to find the correlation between categorical variables. And the result of Cramer-V can also be visualised using heat-map.once we checked the correlation between categorical columns using cramer-v correlation matrix,we can further explore correlation between any two categorical columns and this can be done using a bubble plot between two columns with size of the bubble indicating the number of occurrences. 
    
6)
If database is relatively small every data points counts.In this missing data points means loss of value valuable information.missing data creates inbalanced observation,caused biased estimates and in extreme cases can even lead to invalid conclusions.

Their are some methods to overcome from this problem:
-Use deletation methods to eliminate missing data
-Use regression analysis to systematically eliminate data
-data imputation technique
-bY using above technique we can overcome from this problem.

7)
various methods for dealing with missing data values:
Mean or median imputation .when data is missing at random,we can use list wise or pair wise deletion of the missing observation.
Multivariate imputation by chained equation (MICE)MISC assumes that the missing data are missing at random.
-Random forest.
    
8)
Various data preprocessing techniques:
Data cleaning
Data integration
Data transformation
Data reduction

-Dimensionality reduction=
Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.
These technique can be used in applied machine learning to simplify a classification or regression dataset in order to better fit a predictive model.

-Function selection=
Function selection is a part of data preprocessing which is considered to be the most time consuming parts any machine learning pipeline.
this technique will help you to approch it in a more systematic way and machine learning friendly way.you will be able to interpret the function more accurately.
    
9)
i)
-IQR is interquartile range.It is the range between the first and third quartile namely Q1 and Q3.
-IQR represents how far apart the lowest and highest measurenments were that week.

-Criteria use to access it:
The interquartile range is calculated in much the same way as the range.all you do to find it is substract the first quartile from the third quartile.the interquartile range shows how the data is spread about the median.

ii)
-Various component of box plate:
Median
Quartiles
Whiskers
Fences
Outliers

10)
i)Data collected at regular intervals:
It is also called as integer.It is defined as a data type which is measured along a scale,in which each point is placed at equal distance from one another.
this data always apperas in the form of numbers or numerical value where the distance between the two points is equal.
Technique use in collecting interval data includes observations,interviews,document reviews,surveys and probability sampling.
    
ii)The gap between the quartiles:
In a set of data,the quartiles are the values that divide the data into the four equal parts.the median of a set of data separates the self in half.
The IQR is the range of the middle half of a set of data.it is difference between the upper quartile and lower quartile.
The percentiles and quartiles are related in the sense that the lower quartile (Q1) equals the 25th percentile the middle quartile(Q2) equal to the 50th percentile while the upper quartile equals to the 75th percentile.
    
11)
i)
Nominal and ordinal are two of the four levels of measurement. Nominal level data can only be classified, while ordinal level data can be classified and ordered.also nominal data is classified without a natural order or rank whereas ordinal data has a predetermined or natural order.

ii)
Histograms and box plots are very similar in that they both help to visualize and describe numeric data.although histograms are better in determining the underlying distribution of the data box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space.

iii)
The average of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set whereas the median is the middle value when a data set is ordered from least to greatest.


    