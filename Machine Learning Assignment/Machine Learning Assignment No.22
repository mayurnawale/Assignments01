1)
We can combine these models into a voting ensemble to try to get better results.this works better on models that are very different and trained on different training instances,but it will still work as long as the models are very different.

2)
Hard Voting Classifiers=
Counts the votes of each classifier in the ensemble and picks the class that gets the most votes.

Soft Voting Classifiers=
computes the average estimated class probability for each class and picks the class with the highest probability.it gives high-confidence votes more weight,but only works if every classifier is able to estimate class probabilities.

3)
It is possible to speed up training of bagging, pasting, and random forests because each predictor is independent of the others.
boosting ensemble predictors are built based on the previous predictor, so you separating training will not help.
For stacking ensembles,all predictors for one layer has to be trained before the next layer.

4)
Each predictor in a bagging ensemble is evaluated using instances it was not trained on.this creates an unbiased eval of the ensemble without the need for a validation set.we can have more instances for training, and our ensemble can perform slightly better.

5)
Extra-trees use random thresholds for each feature, instead of finding the best threshold.the extra randomness acts like a form of regularization and can be used to fix overfitting.however,this is neither faster nor slower than random forests when making decisions.

6)
- Increase estimators
- Increase Learning Rate
- Decrease regularization hyperparameters.

7)
Try decreasing the learning rate or use early stopping to find the right number of predictors (there are probably too many)